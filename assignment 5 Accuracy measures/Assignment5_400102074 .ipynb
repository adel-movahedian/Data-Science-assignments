{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "# Assignment 5\n",
        "##  _Adel Movahedian_\n",
        "## **400102074**  \n",
        "## _Sharif University of Technology_\n",
        "\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "7TwUTyBizegU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Accuracy Measures Using MNIST\n",
        "\n",
        "In this notebook we perform the following tasks with the MNIST dataset:\n",
        "\n",
        "1. **Regression Accuracy Metrics:**  \n",
        "   - We treat the digit label (0–9) as a continuous variable.\n",
        "   - We fit a linear regression model to predict the digit.\n",
        "   - We compute:\n",
        "     - Mean Squared Error (MSE)\n",
        "     - Mean Absolute Error (MAE)\n",
        "     - Mean Absolute Percentage Error (MAPE)\n",
        "     - R² Score\n",
        "\n",
        "2. **Binary Classification Accuracy Metrics:**  \n",
        "   - We create a binary classification task by labeling each digit as **even** (1) or **odd** (0).\n",
        "   - We fit a logistic regression model.\n",
        "   - We compute:\n",
        "     - Precision\n",
        "     - Recall\n",
        "     - F1-Score\n",
        "\n",
        "3. **Multi-class Classification Accuracy Metrics:**  \n",
        "   - We use the original 10 classes (digits 0–9).\n",
        "   - We fit a multinomial logistic regression model.\n",
        "   - We compute:\n",
        "     - Class-specific precision and recall\n",
        "     - Macro, Weighted, and Micro-averaged F1-Scores\n",
        "\n",
        "4. **Multi-label Classification Simulation:**  \n",
        "   - We simulate a multi-label scenario by deriving four binary attributes for each sample:\n",
        "     - **Label 1:** Digit is even.\n",
        "     - **Label 2:** Digit is a prime number (2, 3, 5, or 7).\n",
        "     - **Label 3:** Digit is less than 5.\n",
        "     - **Label 4:** Digit is greater than 5.\n",
        "   - We train a multi-label classifier (using One-vs-Rest strategy) and compute:\n",
        "     - Hamming Loss\n",
        "     - Sample-averaged F1-Score\n",
        "\n",
        "Each cell includes explanations to clarify the purpose and findings.\n"
      ],
      "metadata": {
        "id": "lKKMhuSGz1GQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries and Load MNIST"
      ],
      "metadata": {
        "id": "yTBwqljCz65g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwXOOxd4zHgO",
        "outputId": "980726bf-7d98-412c-fc07-4caa0f54239f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST dataset loaded:\n",
            "Shape of X: (10000, 784)\n",
            "Shape of y: (10000,)\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import (mean_squared_error, mean_absolute_error,\n",
        "                             mean_absolute_percentage_error, r2_score,\n",
        "                             precision_score, recall_score, f1_score,\n",
        "                             hamming_loss)\n",
        "\n",
        "# Load the MNIST dataset from OpenML\n",
        "# Note: This may take a moment as the dataset is large.\n",
        "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
        "X = mnist.data    # 70000 samples, 784 features (28x28 images flattened)\n",
        "X = X[:10000]\n",
        "y = mnist.target.astype(int)  # Convert string labels to integers\n",
        "y = y[:10000]\n",
        "# Print basic information\n",
        "print(\"MNIST dataset loaded:\")\n",
        "print(\"Shape of X:\", X.shape)\n",
        "print(\"Shape of y:\", y.shape)\n",
        "\n",
        "# Explanation:\n",
        "# - We load the MNIST dataset using fetch_openml.\n",
        "# - The features (X) represent pixel intensities and y contains the digit labels (0–9).\n",
        "# - We choose the first 10000 data for process\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression Task Using MNIST"
      ],
      "metadata": {
        "id": "NMR1wUpQ0EbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For regression, we treat the digit label as a continuous variable.\n",
        "# Split the data into training and testing sets (using 20% of the data for testing)\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train a linear regression model\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Predict the digit labels (as continuous values) on the test set\n",
        "y_pred_reg = regressor.predict(X_test_reg)\n",
        "\n",
        "# Compute regression metrics\n",
        "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
        "mae = mean_absolute_error(y_test_reg, y_pred_reg)\n",
        "mape = mean_absolute_percentage_error(y_test_reg, y_pred_reg)\n",
        "r2 = r2_score(y_test_reg, y_pred_reg)\n",
        "\n",
        "# Display the results\n",
        "print(\"Regression Metrics (Predicting digit as continuous value):\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.4f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")\n",
        "\n",
        "# Explanation:\n",
        "# - We use a linear regression model to predict the digit label.\n",
        "# - Although digit labels are discrete, treating them as continuous allows us to compute standard regression metrics.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWw2l4n60F33",
        "outputId": "5cc793d2-c92d-41b3-8d85-b0bb78f9eb7a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regression Metrics (Predicting digit as continuous value):\n",
            "Mean Squared Error (MSE): 4.6868\n",
            "Mean Absolute Error (MAE): 1.4820\n",
            "Mean Absolute Percentage Error (MAPE): 666409616012723.8750\n",
            "R² Score: 0.4281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color= \"green\"> these regression metrics indicate that the regression approach (using a linear regression model on raw MNIST images for predicting digit values) is not suitable or effective with the given data subset. For MNIST, using models tailored for classification (as We did for the other parts) is the proper approach, and that's why those parts yield reasonable results."
      ],
      "metadata": {
        "id": "8ohexEuF4L1i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binary Classification Task Using MNIST"
      ],
      "metadata": {
        "id": "rB2f2cc20Ifr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For binary classification, we label digits as even (1) and odd (0)\n",
        "y_binary = (y % 2 == 0).astype(int)  # 1 for even, 0 for odd\n",
        "\n",
        "# Split the data (80% training, 20% testing)\n",
        "X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features to help with convergence\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_bin = scaler.fit_transform(X_train_bin)\n",
        "X_test_bin = scaler.transform(X_test_bin)\n",
        "\n",
        "# Initialize and train a logistic regression classifier with increased max_iter\n",
        "clf_bin = LogisticRegression(max_iter=20000, random_state=42)\n",
        "clf_bin.fit(X_train_bin, y_train_bin)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_bin = clf_bin.predict(X_test_bin)\n",
        "\n",
        "# Compute binary classification metrics\n",
        "precision_bin = precision_score(y_test_bin, y_pred_bin)\n",
        "recall_bin = recall_score(y_test_bin, y_pred_bin)\n",
        "f1_bin = f1_score(y_test_bin, y_pred_bin)\n",
        "\n",
        "# Display the results\n",
        "print(\"Binary Classification Metrics (Even vs. Odd):\")\n",
        "print(f\"Precision: {precision_bin:.4f}\")\n",
        "print(f\"Recall: {recall_bin:.4f}\")\n",
        "print(f\"F1-Score: {f1_bin:.4f}\")\n",
        "\n",
        "# Explanation:\n",
        "# - We convert the digit labels into a binary format: even (1) and odd (0).\n",
        "# - Scaling the data helps the optimization algorithm converge faster and more reliably.\n",
        "# - Increasing max_iter to 20000 ensures that the solver has enough iterations to converge.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUSQyxO50Mlu",
        "outputId": "0dd287cb-66d7-4fb4-8e91-6b05c91988cb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary Classification Metrics (Even vs. Odd):\n",
            "Precision: 0.8978\n",
            "Recall: 0.8710\n",
            "F1-Score: 0.8842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-class Classification Task Using MNIST"
      ],
      "metadata": {
        "id": "wmh9addw0O7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For multi-class classification, we use the original digit labels (0-9)\n",
        "# Split the data (80% training, 20% testing)\n",
        "X_train_mc, X_test_mc, y_train_mc, y_test_mc = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train a logistic regression classifier\n",
        "# Removed the 'multi_class' parameter to avoid the FutureWarning.\n",
        "clf_mc = LogisticRegression(solver='lbfgs', max_iter=2000, random_state=42)\n",
        "clf_mc.fit(X_train_mc, y_train_mc)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_mc = clf_mc.predict(X_test_mc)\n",
        "\n",
        "# Compute class-specific precision and recall (returned as an array for each class)\n",
        "precision_each = precision_score(y_test_mc, y_pred_mc, average=None)\n",
        "recall_each = recall_score(y_test_mc, y_pred_mc, average=None)\n",
        "\n",
        "# Compute aggregated F1-scores using different averaging methods\n",
        "f1_macro = f1_score(y_test_mc, y_pred_mc, average='macro')\n",
        "f1_weighted = f1_score(y_test_mc, y_pred_mc, average='weighted')\n",
        "f1_micro = f1_score(y_test_mc, y_pred_mc, average='micro')\n",
        "\n",
        "# Display the results\n",
        "print(\"Multi-class Classification Metrics (Digits 0-9):\")\n",
        "print(f\"Precision for each class: {precision_each}\")\n",
        "print(f\"Recall for each class: {recall_each}\")\n",
        "print(f\"Macro-averaged F1-Score: {f1_macro:.4f}\")\n",
        "print(f\"Weighted-averaged F1-Score: {f1_weighted:.4f}\")\n",
        "print(f\"Micro-averaged F1-Score: {f1_micro:.4f}\")\n",
        "\n",
        "# Explanation:\n",
        "# - We train a logistic regression model to classify all 10 digits.\n",
        "# - Class-specific precision and recall help evaluate performance for each digit.\n",
        "# - Aggregated F1-scores (macro, weighted, micro) summarize overall performance.\n",
        "# - By removing the 'multi_class' parameter, we avoid the FutureWarning while retaining the default multinomial behavior.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xwx1TSsu0Sol",
        "outputId": "a26a39f2-6bf0-47cc-fcfe-ab3fe6909f92"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi-class Classification Metrics (Digits 0-9):\n",
            "Precision for each class: [0.9478673  0.93693694 0.86528497 0.83       0.93170732 0.83050847\n",
            " 0.9321267  0.88181818 0.78658537 0.86096257]\n",
            "Recall for each class: [0.96618357 0.96296296 0.81862745 0.86458333 0.90521327 0.83522727\n",
            " 0.93636364 0.89814815 0.77710843 0.83854167]\n",
            "Macro-averaged F1-Score: 0.8802\n",
            "Weighted-averaged F1-Score: 0.8842\n",
            "Micro-averaged F1-Score: 0.8845\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-label Classification Simulation Using MNIST"
      ],
      "metadata": {
        "id": "-mm-EYIX0Upf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For the multi-label classification simulation, we derive 4 binary attributes from each digit:\n",
        "# Label 1: Digit is even.\n",
        "# Label 2: Digit is prime (prime digits: 2, 3, 5, 7).\n",
        "# Label 3: Digit is less than 5.\n",
        "# Label 4: Digit is greater than 5.\n",
        "\n",
        "# Define helper functions for each attribute\n",
        "def is_even(x):\n",
        "    return int(x % 2 == 0)\n",
        "\n",
        "def is_prime(x):\n",
        "    return int(x in [2, 3, 5, 7])\n",
        "\n",
        "def less_than_5(x):\n",
        "    return int(x < 5)\n",
        "\n",
        "def greater_than_5(x):\n",
        "    return int(x > 5)\n",
        "\n",
        "# Create multi-label targets for all samples\n",
        "y_multilabel = np.column_stack((\n",
        "    np.array([is_even(val) for val in y]),\n",
        "    np.array([is_prime(val) for val in y]),\n",
        "    np.array([less_than_5(val) for val in y]),\n",
        "    np.array([greater_than_5(val) for val in y])\n",
        "))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train_ml, X_test_ml, y_train_ml, y_test_ml = train_test_split(X, y_multilabel, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features to help with convergence\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler_ml = StandardScaler()\n",
        "X_train_ml = scaler_ml.fit_transform(X_train_ml)\n",
        "X_test_ml = scaler_ml.transform(X_test_ml)\n",
        "\n",
        "# For multi-label classification, we use One-vs-Rest strategy with logistic regression\n",
        "clf_ml = OneVsRestClassifier(LogisticRegression(max_iter=20000, random_state=42))\n",
        "clf_ml.fit(X_train_ml, y_train_ml)\n",
        "\n",
        "# Predict multi-label outputs on the test set\n",
        "y_pred_ml = clf_ml.predict(X_test_ml)\n",
        "\n",
        "# Compute multi-label metrics\n",
        "hloss = hamming_loss(y_test_ml, y_pred_ml)\n",
        "f1_ml = f1_score(y_test_ml, y_pred_ml, average='samples')\n",
        "\n",
        "# Display the results\n",
        "print(\"Multi-label Classification Metrics (Simulated Attributes from MNIST):\")\n",
        "print(f\"Hamming Loss: {hloss:.4f}\")\n",
        "print(f\"Sample-averaged F1-Score: {f1_ml:.4f}\")\n",
        "\n",
        "# Explanation:\n",
        "# - We derive four binary labels from the digit label based on different criteria.\n",
        "# - We scale the features to improve the convergence of the logistic regression solver.\n",
        "# - Increasing max_iter to 20000 gives the solver more iterations to converge.\n",
        "# - The Hamming Loss indicates the fraction of labels misclassified, while the sample-averaged F1-Score balances precision and recall for each sample.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTfVzT550aLa",
        "outputId": "f8b52808-f85e-4d60-9bfe-06ac73e8423a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi-label Classification Metrics (Simulated Attributes from MNIST):\n",
            "Hamming Loss: 0.1212\n",
            "Sample-averaged F1-Score: 0.8471\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Remarks\n",
        "\n",
        "In this notebook we adapted the MNIST dataset to perform several tasks:\n",
        "\n",
        "1. **Regression:**  \n",
        "   - Treated the digit label as a continuous variable.\n",
        "   - Computed MSE, MAE, MAPE, and R² Score using linear regression.\n",
        "\n",
        "2. **Binary Classification:**  \n",
        "   - Defined a binary task (even vs. odd digits).\n",
        "   - Evaluated performance with precision, recall, and F1-Score.\n",
        "\n",
        "3. **Multi-class Classification:**  \n",
        "   - Used the original 10-class problem.\n",
        "   - Computed class-specific precision/recall and overall F1-Scores (macro, weighted, micro).\n",
        "\n",
        "4. **Multi-label Classification Simulation:**  \n",
        "   - Generated four artificial binary labels from the digit information.\n",
        "   - Employed a One-vs-Rest logistic regression classifier.\n",
        "   - Measured performance using Hamming Loss and sample-averaged F1-Score.\n",
        "\n",
        "This approach demonstrates that MNIST can be creatively used for various accuracy measure evaluations, even if it was originally designed for digit recognition.\n"
      ],
      "metadata": {
        "id": "_wAKxCgL0hRi"
      }
    }
  ]
}