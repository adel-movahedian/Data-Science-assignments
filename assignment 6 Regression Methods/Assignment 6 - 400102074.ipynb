{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4APzt2-n0FRO"
      },
      "source": [
        "# Adel Movahedian \n",
        "# 400102074\n",
        "## assignment 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13viOV08lkUV",
        "outputId": "a92dad87-b018-46f8-b8d1-5c2dcd9b61e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       Country  Year      Status  Life expectancy   Adult Mortality  \\\n",
            "0  Afghanistan  2015  Developing              65.0            263.0   \n",
            "1  Afghanistan  2014  Developing              59.9            271.0   \n",
            "2  Afghanistan  2013  Developing              59.9            268.0   \n",
            "3  Afghanistan  2012  Developing              59.5            272.0   \n",
            "4  Afghanistan  2011  Developing              59.2            275.0   \n",
            "\n",
            "   infant deaths  Alcohol  percentage expenditure  Hepatitis B  Measles   ...  \\\n",
            "0             62     0.01               71.279624         65.0      1154  ...   \n",
            "1             64     0.01               73.523582         62.0       492  ...   \n",
            "2             66     0.01               73.219243         64.0       430  ...   \n",
            "3             69     0.01               78.184215         67.0      2787  ...   \n",
            "4             71     0.01                7.097109         68.0      3013  ...   \n",
            "\n",
            "   Polio  Total expenditure  Diphtheria    HIV/AIDS         GDP  Population  \\\n",
            "0    6.0               8.16         65.0        0.1  584.259210  33736494.0   \n",
            "1   58.0               8.18         62.0        0.1  612.696514    327582.0   \n",
            "2   62.0               8.13         64.0        0.1  631.744976  31731688.0   \n",
            "3   67.0               8.52         67.0        0.1  669.959000   3696958.0   \n",
            "4   68.0               7.87         68.0        0.1   63.537231   2978599.0   \n",
            "\n",
            "    thinness  1-19 years   thinness 5-9 years  \\\n",
            "0                   17.2                 17.3   \n",
            "1                   17.5                 17.5   \n",
            "2                   17.7                 17.7   \n",
            "3                   17.9                 18.0   \n",
            "4                   18.2                 18.2   \n",
            "\n",
            "   Income composition of resources  Schooling  \n",
            "0                            0.479       10.1  \n",
            "1                            0.476       10.0  \n",
            "2                            0.470        9.9  \n",
            "3                            0.463        9.8  \n",
            "4                            0.454        9.5  \n",
            "\n",
            "[5 rows x 22 columns]\n",
            "['Country', 'Year', 'Status', 'Life expectancy', 'Adult Mortality', 'infant deaths', 'Alcohol', 'percentage expenditure', 'Hepatitis B', 'Measles', 'BMI', 'under-five deaths', 'Polio', 'Total expenditure', 'Diphtheria', 'HIV/AIDS', 'GDP', 'Population', 'thinness  1-19 years', 'thinness 5-9 years', 'Income composition of resources', 'Schooling']\n",
            "\n",
            "Linear Regression R2 Score: 0.907\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, r2_score\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "## -----------------------------------------\n",
        "\n",
        "df = pd.read_csv('omid_zendegi.csv')\n",
        "print(df.head())\n",
        "\n",
        "df.columns = df.columns.str.strip()\n",
        "print(df.columns.tolist())\n",
        "\n",
        "df = df.dropna(subset=['Life expectancy'])\n",
        "\n",
        "df['Is_Developed'] = df['Status'].map({'Developed': 1, 'Developing': 0})\n",
        "df = df.drop(columns=['Status', 'Country'])\n",
        "\n",
        "numerical_features = [\n",
        "    'Year', 'Is_Developed', 'Adult Mortality', 'infant deaths', 'Alcohol',\n",
        "    'percentage expenditure', 'Hepatitis B', 'Measles', 'BMI', 'under-five deaths',\n",
        "    'Polio', 'Total expenditure', 'Diphtheria', 'HIV/AIDS', 'GDP', 'Population',\n",
        "    'thinness  1-19 years', 'Income composition of resources', 'Schooling'\n",
        "]\n",
        "\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "df[numerical_features] = imputer.fit_transform(df[numerical_features])\n",
        "\n",
        "df['log_GDP'] = np.log(df['GDP'] + 1)\n",
        "df['log_Population'] = np.log(df['Population'] + 1)\n",
        "df = df.drop(columns=['GDP', 'Population'])\n",
        "df['Alcohol_squared'] = df['Alcohol'] ** 2\n",
        "\n",
        "features = [\n",
        "    'Year', 'Is_Developed', 'Adult Mortality', 'infant deaths', 'Alcohol',\n",
        "    'Alcohol_squared', 'percentage expenditure', 'Hepatitis B', 'Measles', 'BMI',\n",
        "    'Polio', 'Total expenditure', 'Diphtheria', 'HIV/AIDS', 'log_GDP',\n",
        "    'log_Population', 'thinness  1-19 years', 'Income composition of resources',\n",
        "    'Schooling'\n",
        "]\n",
        "\n",
        "X = df[features]\n",
        "y = df['Life expectancy']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_train_poly = poly.fit_transform(X_train_scaled)\n",
        "X_test_poly = poly.transform(X_test_scaled)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_poly, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test_poly)\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"\\nLinear Regression R2 Score: {r2:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code cleans and processes life expectancy data, converts categorical values into numbers, transforms features, and handles missing values. It prepares the data, trains a linear regression model, and evaluates its performance using the R2 score to assess prediction accuracy.\n",
        "\n",
        "this code provides a practical demonstration of data preprocessing and regression analysis in machine learning. It highlights how to clean, transform, and encode real-world data while addressing missing values effectively. I gained an understanding of feature transformation techniques (like logarithmic scaling and polynomial expansion) to capture relationships in data, and how standardization ensures consistent scaling. Moreover, the code illustrates the importance of dividing data into training and testing sets for unbiased model evaluation, and how to use metrics like R2 to assess prediction accuracy. It’s a concise yet complete workflow for predictive modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3B8dfWkoqPw8",
        "outputId": "2a08dacb-6290-4779-a8fb-70cfdf12c38f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kernel Regression R2 Score: 0.923\n"
          ]
        }
      ],
      "source": [
        "param_grid = {\n",
        "    'alpha': [0.01, 0.1, 1, 10],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1],\n",
        "    'kernel': ['rbf', 'poly']\n",
        "}\n",
        "\n",
        "kr = KernelRidge()\n",
        "\n",
        "grid_search = GridSearchCV(kr, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_kernel_opt = grid_search.predict(X_test_scaled)\n",
        "r2_kernel_opt = r2_score(y_test, y_pred_kernel_opt)\n",
        "print(f\"Kernel Regression R2 Score: {r2_kernel_opt:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This part of the code aims to optimize the performance of a Kernel Ridge Regression model for predicting life expectancy. By using GridSearchCV, it systematically tests different combinations of hyperparameters (alpha, gamma, and kernel types like 'rbf' and 'poly') to find the best configuration that maximizes the R2 score. The process uses cross-validation (cv=5) to ensure the model's performance is evaluated robustly and avoids overfitting to the training data.\n",
        "\n",
        "The optimized model is then applied to the test data, and its accuracy is measured using the R2 score, which indicates how well the optimized model explains the variation in life expectancy.\n",
        "\n",
        "From this, I have gained an understanding of hyperparameter tuning's importance in improving model accuracy and robustness. It also demonstrates the use of cross-validation to validate model performance effectively. By automating the search for the best parameters, it ensures the model achieves its optimal potential without relying on arbitrary or manual parameter selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLA21Bcwq5pk",
        "outputId": "6de97746-6eae-4fe3-9020-7694068834f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression R2 Score: 0.810\n"
          ]
        }
      ],
      "source": [
        "df['High_life'] = (df['Life expectancy'] >= 70).astype(int)\n",
        "\n",
        "selected_features = [\n",
        "    'Year', 'Is_Developed', 'Adult Mortality', 'Alcohol', 'BMI',\n",
        "    'HIV/AIDS', 'log_GDP', 'log_Population', 'Income composition of resources',\n",
        "    'Schooling', 'Total expenditure', 'percentage expenditure', 'Diphtheria', 'Polio'\n",
        "]\n",
        "\n",
        "X_log = df[selected_features]\n",
        "y_log = df['High_life']\n",
        "\n",
        "X_train_log, X_test_log, y_train_log, y_test_log = train_test_split(X_log, y_log, test_size=0.2, random_state=42)\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('poly', PolynomialFeatures(degree=4, interaction_only=False, include_bias=False)),\n",
        "    ('logreg', LogisticRegression(C=100, solver='liblinear', max_iter=1000, class_weight='balanced'))\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train_log, y_train_log)\n",
        "\n",
        "y_pred = pipeline.predict(X_test_log)\n",
        "y_pred_prob = pipeline.predict_proba(X_test_log)[:, 1]\n",
        "\n",
        "accuracy = accuracy_score(y_test_log, y_pred)\n",
        "r2 = r2_score(y_test_log, y_pred_prob)\n",
        "\n",
        "print(f\"Logistic Regression R2 Score: {r2:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "This part of the code builds and evaluates a logistic regression model to classify countries into two groups based on their life expectancy: countries with high life expectancy (≥70 years) and those below 70. It creates a new binary column (High_life) in the dataset to represent this classification.\n",
        "\n",
        "To prepare for modeling, the code selects relevant features and splits the data into training and testing sets. Then, it creates a pipeline that standardizes the features, generates polynomial features for capturing complex interactions, and applies logistic regression for classification. The pipeline is trained on the training set and evaluated on the test set. Predictions are made both for the classes and probabilities, with the model's performance assessed using accuracy and the R2 score.\n",
        "\n",
        "This taught me how to transform a regression task into a classification problem by defining custom thresholds. I also learned how pipelines simplify preprocessing and modeling, ensuring all steps are applied consistently. Using polynomial features and balanced class weights helps improve performance, especially with imbalanced datasets. Additionally, the R2 score for probabilities demonstrates how well the model predicts the likelihood of high life expectancy rather than just its binary outcome.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ipm9MZQzVTB",
        "outputId": "a4ce53a9-8bc2-4be2-8187-6ae0599c2c97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ridge Regression R2 Score: 0.907\n"
          ]
        }
      ],
      "source": [
        "ridge_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('poly', PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)),\n",
        "    ('ridge', Ridge(alpha=10))\n",
        "])\n",
        "\n",
        "ridge_pipeline.fit(X_train, y_train)\n",
        "y_pred_ridge = ridge_pipeline.predict(X_test)\n",
        "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
        "\n",
        "print(f\"Ridge Regression R2 Score: {r2_ridge:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "This part of the code uses Ridge Regression, a linear model with regularization to prevent overfitting, particularly useful when dealing with multicollinearity or polynomial features. The pipeline standardizes the data, creates polynomial features to capture non-linear relationships, and applies Ridge Regression with a specified regularization strength (alpha=10).\n",
        "\n",
        "The model is trained on the training set and evaluated on the test set using the R2 score to measure how well it predicts the target variable (life expectancy). By introducing polynomial features and regularization, the goal is to balance complexity and performance, avoiding overfitting while capturing meaningful interactions.\n",
        "\n",
        "From this, I’ve learned how Ridge Regression can be effectively combined with feature transformations to handle complex relationships in data. It demonstrates how regularization helps control model complexity, making it both robust and generalizable to unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pe5syAyVzhSF",
        "outputId": "b7e1aa35-c5d6-449d-c4ca-a704049108a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LASSO Regression R2 Score: 0.908\n"
          ]
        }
      ],
      "source": [
        "lasso_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('poly', PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)),\n",
        "    ('lasso', Lasso(alpha=0.01, max_iter=5000))\n",
        "])\n",
        "\n",
        "lasso_pipeline.fit(X_train, y_train)\n",
        "y_pred_lasso = lasso_pipeline.predict(X_test)\n",
        "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
        "\n",
        "print(f\"LASSO Regression R2 Score: {r2_lasso:.3f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This section implements LASSO regression, a linear model that applies regularization to enhance the selection of relevant features by penalizing less impactful ones. The pipeline begins by standardizing the data to ensure all features have a uniform scale, adds polynomial features to capture nonlinear relationships, and then applies the LASSO regression with specific regularization (alpha=0.01). This method is particularly useful for simplifying models by shrinking less influential feature coefficients to zero.\n",
        "\n",
        "The model is trained on the training data and tested on the test set. Its performance is measured using the R2 score, which indicates how well the model predicts the target variable (life expectancy).\n",
        "\n",
        "From this, I’ve learned how LASSO regression is effective for reducing model complexity by performing feature selection, ensuring robustness without overfitting. It also demonstrates the combination of polynomial features and regularization to capture intricate data relationships while maintaining simplicity and interpretability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCESQdFG39bK"
      },
      "source": [
        "The kernel trick is a method used in machine learning to efficiently map input data to a higher-dimensional space without explicitly performing the transformation. Instead of calculating the mapping, it uses a kernel function to compute the relationship between data points in the transformed space directly. This enables models, such as Kernel Ridge Regression or Support Vector Machines, to capture complex, nonlinear patterns in the data.\n",
        "\n",
        "By using the kernel trick, regression models can better fit complicated relationships within the data, leading to improved prediction accuracy, especially in cases where linear models fall short. It's computationally efficient and avoids the high costs of working in very high-dimensional spaces explicitly."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
